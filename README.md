# Notes: Generative AI with Large Language Models

Welcome to my repository for notes from the **Generative AI with Large Language Models** course by Andrew Ng!  

This repo is a comprehensive collection of my learnings and personal insights I gained as I progress through this course. It is designed to be a reference for myself and anyone interested in learning about Generative AI and Large Language Models (LLMs).  

---

## üìö Course Overview  

The **Generative AI with Large Language Models** course explores:  
- The foundational principles behind generative AI.  
- Techniques for training and fine-tuning LLMs.  
- Practical applications of LLMs in various domains.  
- Ethical considerations in deploying AI systems.  

This course is offered by [DeepLearning.AI](https://www.deeplearning.ai/) and led by **Andrew Ng**.  

---

## ü§ù Contributions  

While this is primarily a personal learning repository, contributions are welcome! Feel free to:  
- Suggest improvements to the notes.  
- Share additional resources or ideas in the Issues tab.  
- Fork the repo and explore your own LLM-related projects!  

---

## üìß Contact  

If you have any questions, suggestions, or feedback, feel free to reach out:  
- **GitHub**: [RuvenGuna94](https://github.com/RuvenGuna94)  
- **Email**: ruvenguna94@hotmail.com

---

## üßë‚Äçüíª About Me  

I am a Solution Engineer with a strong interest in **Artificial Intelligence**, particularly in **Generative AI**. My work involves designing AI-driven solutions for enterprises, enabling AI adoption at scale, and helping organizations leverage the power of advanced machine learning models.  

---

## üåê Acknowledgments  

A big thank you to **Andrew Ng** and [DeepLearning.AI](https://www.deeplearning.ai/) for this excellent course and for inspiring learners worldwide to dive deep into AI!  

---

## üìù License  

This repository is open-source and available under the MIT License.  

---

# üõ†Ô∏è Developer Log

## 1 Dec 2024

- Created notes folder
- Added in images to IMG folder
- Ported all ntotes from One Note

## 17 December 2024
- Uploaded notes for pre-training and model size

## 19 December 2024
- Uploaded notes for Distributed Training

## 22 December 2024
- Uploaded notes for Scaling laws and compute-optimal models
- Uploaded notes for Instruction Fine-Tuning

## 23 December 2024
- Uploaded notes for Multi-task instruction fine-tuning

## 27 December 2024
- Updated images to note 8

## 28 December 2024
- Updated Benchmarks
- Updated Parameter efficient fine-tuning (PEFT)
- Updated PEFT Technique: Low-Rank Adaptation (LoRA)

## 30 December 2024
- Updated PEFT Technique: Soft Prompts - Prompt Tuning

## 31 December 2024
- Updated Reinforcement Learning from Human Feedback (RLHF)
- Updated 14. RLHF - Obtaining feedback from humans

## 1 Jan 2025
- Updated RLHF - Reward model
- Updated RLHF - Fine-tuning with reinforcement learning

## 5 Jan 2025
- Updated Proximal policy optimization
- Updated RLHF - Reward hacking
- Updated Reward model - Scaling human feedback
- Updated Model optimizations for deployment

## 7 Jan 2025
- Updated Generative AI Project Lifecycle Cheat Sheet
- Updated Using the LLM in applications
- Updated Interacting with external applications

## 8 Jan 2025
- Updated Helping LLMs reason and plan with chain-of-thought
- Updated Program-aided language models (PAL)

## 9 Jan 2025
- Updated ReAct: Combining reasoning and action
- Updated LLM application architectures