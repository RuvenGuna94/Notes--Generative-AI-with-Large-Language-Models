# Introduction
- **Goal**: Improve model performance without altering the original model weights.
- **Contrast with Prompt Engineering**:
  - **Prompt Engineering**:
    - Manually crafting prompts to guide the model.
    - Can be time-consuming and may not always achieve desired results.
  - **Prompt Tuning**:
    - Adds trainable "soft prompts" to the input.
    - Leverages supervised learning to optimize the soft prompts for the specific task.

---

# Soft Prompts
- **Definition**:
  - A set of trainable tokens prepended to the input text.
  - Have the same length as the embedding vectors of input tokens.
- **Key Characteristics**:
  - **Virtual Tokens**:
    - Not fixed words; can take on any value within the embedding space.
    - Learned during the training process.
- **Training**:
  - **Frozen Model Weights**: The LLM's weights remain unchanged.
  - **Soft Prompt Optimization**: Only the embedding vectors of the soft prompt are updated during training.

---

## Full Fine-Tuning vs Soft Prompts
- In full fine-tuning, the weights of the large language model are updated during supervised learning.
- In contrast, with prompt tuning, the weights of the large language model are frozen, and the underlying model does not get updated.
- Instead, the embedding vectors of the soft prompt are updated over time to optimize the model's completion of the prompt.
- Prompt tuning is a very parameter-efficient strategy because only a few parameters are being trained, in contrast with the millions to billions of parameters in full fine-tuning.
- You can train a different set of soft prompts for each task and then easily swap them out at inference time.

# Advantages
- **Parameter Efficiency**:
  - Trains only a small number of parameters compared to full fine-tuning.
- **Flexibility**:
  - Easily switch between tasks by swapping out the learned soft prompts.
- **Minimal Storage**:
  - Soft prompts are small in size, requiring minimal disk storage.

# Performance
- **Comparable to Full Fine-Tuning**:
  - Prompt tuning doesn't perform as well as full fine-tuning for smaller LLMs.
  - For larger models (e.g., 10 billion parameters), prompt tuning can achieve performance comparable to full fine-tuning on many tasks.
- **Superior to Prompt Engineering**:
  - Offers significant performance improvements over traditional prompt engineering.
- **Limited Interpretability**:
  - Learned soft prompt tokens may not directly correspond to human-readable words or phrases.
  - Analyzing the nearest neighbor tokens can provide insights into the learned representations.

