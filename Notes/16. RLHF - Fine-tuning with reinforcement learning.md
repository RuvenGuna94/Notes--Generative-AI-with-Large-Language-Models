# Utilizing the Reward Model for Fine-Tuning

## 1. Starting with a Pre-Trained Model
- Begin with an LLM that already performs well on the task of interest (e.g., instruction following, question answering). The goal is to align the model with human values.

## 2. Passing Prompts through the LLM
- For example, the prompt "A dog is" is passed to an instruct LLM, and it generates a response, like "a furry animal."

## 3. Evaluating with the Reward Model
- The prompt and its generated completion are sent to the reward model.
- The reward model evaluates the response based on human feedback and returns a reward value. 
  - Higher values (e.g., 0.24) indicate more aligned responses.
  - Lower values (e.g., -0.53) suggest misalignment.

## 4. Updating the LLM Using the Reward Value
- The reward value is passed to the reinforcement learning algorithm, which adjusts the LLM's weights to improve alignment with human preferences.
- This updated model is referred to as the RL updated LLM.

## 5. Iterative Process
- This process repeats over multiple iterations (similar to fine-tuning), where each iteration updates the LLM's weights based on the reward value it receives.
- As the iterations progress, the LLM's generated responses should become more aligned with human preferences, reflected by increasing reward scores.

## 6. Stopping Criteria
- The process continues until the model reaches a specific evaluation criterion, such as:
  - Reaching a predefined helpfulness threshold.
  - Completing a set number of iterations (e.g., 20,000 steps).


