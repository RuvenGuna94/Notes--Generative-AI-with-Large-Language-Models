# Utilizing the Reward Model for Fine-Tuning

## 1. Starting with a Pre-Trained Model
- Begin with an LLM that already performs well on the task of interest (e.g., instruction following, question answering). The goal is to align the model with human values.

## 2. Passing Prompts through the LLM
- For example, the prompt "A dog is" is passed to an instruct LLM, and it generates a response, like "a furry animal."

## 3. Evaluating with the Reward Model
- The prompt and its generated completion are sent to the reward model.
- The reward model evaluates the response based on human feedback and returns a reward value. 
  - Higher values (e.g., 0.24) indicate more aligned responses.
  - Lower values (e.g., -0.53) suggest misalignment.


