# Pre-Training a Large Language Model (LLM)

## Key Characteristics
- Most complex stage.
- Involves model architecture decisions, extensive training data, and high expertise.

## Relevance
- Generally skipped when starting with a foundation model.

# Performance Assessment

## Prompt Engineering
- Requires minimal technical expertise.
- No additional model training needed.

## Use Case
- Assess and refine the model's performance without heavy resource investment.

# Model Tuning

## Fine-Tuning Methods
- **Full Fine-Tuning**: Adjusts all parameters for the specific use case.
- **Parameter-Efficient Techniques** (e.g., LoRA, Prompt Tuning): Focuses on a smaller set of parameters to save compute and time.

## Effort and Expertise
- Requires some technical expertise.
- Often successful with a small training dataset.
- Can potentially be completed in a day.
