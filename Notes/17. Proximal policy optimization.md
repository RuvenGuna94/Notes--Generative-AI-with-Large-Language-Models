# Considerations for Deploying LLMs

### Functionality in Deployment
- How fast do you need your model to generate completions?
- What compute budget is available?
- Are you willing to trade performance for inference speed or lower storage?

### External Resources
- Will the model interact with external data or applications?
- How will connections to these resources be managed?

### Consumption of the Model
- What will the application or API interface for the model look like?