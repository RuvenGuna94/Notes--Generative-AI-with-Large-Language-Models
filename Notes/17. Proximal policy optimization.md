# Considerations for Deploying LLMs

### Functionality in Deployment
- How fast do you need your model to generate completions?
- What compute budget is available?
- Are you willing to trade performance for inference speed or lower storage?

### External Resources
- Will the model interact with external data or applications?
- How will connections to these resources be managed?

### Consumption of the Model
- What will the application or API interface for the model look like?

# Challenges of LLM Inference

### Key Issues
- High computing and storage requirements.
- Ensuring low latency for consuming applications.
- Deployment challenges across on-premise, cloud, and edge devices.

### Primary Goal
- Reduce model size while maintaining performance.

# Model Optimization Techniques

## Overview
- Techniques to improve inference performance and reduce resource usage.
- Focus: Distillation, Quantization, and Pruning.