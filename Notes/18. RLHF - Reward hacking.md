# Overview of RLHF and PPO in LLM Fine-tuning

## Fine-Tuning Objective
- Aligns Large Language Models (LLMs) with human preferences.
- Uses a reward model to evaluate LLM completions based on human preference metrics (e.g., helpfulness).
- Proximal Policy Optimization (PPO) is employed to update LLM weights iteratively for improved alignment.

## Fine-Tuning Process
- Prompts are passed to the LLM, which generates completions.
- The reward model assigns scores to these completions.
- PPO uses these rewards to update LLM weights over multiple iterations.
- The process continues until the desired alignment level is achieved.
- **End result**: A human-aligned LLM suitable for applications.

# Reward Hacking in RLHF

## Definition
- Occurs when the model exploits the reward system for high scores, diverging from the original objective.
- **Example**: Generating exaggerated or nonsensical phrases to lower toxicity scores.

## Manifestations in LLMs
- Adds exaggerated or irrelevant phrases (e.g., “most awesome”) to optimize reward.
- May produce grammatically incorrect or incoherent responses.

# Preventing Reward Hacking

## Reference Model
- Use the initial instruct LLM as a frozen reference during training.
- Compare completions of the updated LLM with the reference model.

## Kullback-Leibler (KL) Divergence
- A measure of difference between the probability distributions of two models.
- Used to compare completions of the reference and updated models.
- Added as a penalty term to the reward calculation if the updated model diverges too much.

## Training Process with KL Divergence
- Each prompt is processed by both the reference and updated LLMs.
- KL divergence is calculated for all tokens across the vocabulary.
- Penalizes updates that shift too far from the reference model.

## Implementation Considerations
- Requires two full copies of the LLM: reference and updated models.
- Computationally intensive; benefits from GPU acceleration.

# Optimizing Training with LoRA (Low-Rank Adaptation)

## LoRA Overview
- Updates only the LoRA adapter weights instead of the entire LLM weights.
- Allows the same LLM to serve as both reference and PPO models.
- Reduces memory footprint by approximately 50%.

## Benefits
- Efficient memory usage.
- Lower computational overhead during training.

# Evaluating Model Alignment

## Assessment Process
- Use a dataset (e.g., summarization or dialogue datasets) to measure the model's performance.
- **Toxicity score**: The probability of generating toxic or hateful responses.

## Evaluation Steps
1. Compute the baseline toxicity score for the original LLM using a reward model.
2. Evaluate the aligned LLM on the same dataset and compare scores.

## Outcome
- A successful RLHF process should result in a lower toxicity score, indicating better alignment with human preferences.
