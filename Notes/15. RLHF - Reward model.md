# Training the Reward Model

## Purpose
- The reward model replaces human labelers once it has been trained.
- It automatically selects the preferred completion during the RLHF process.

## Reward Model as a Language Model
- The reward model is typically a language model, trained using supervised learning on the pairwise comparison data from human labelers (e.g., BERT).
- The model learns to favor the human-preferred completion and minimizes the difference between the reward values for different completions.
![image](https://github.com/user-attachments/assets/9ec2d7c1-75d3-48ef-b188-16c44f3b42dc)

---

# Reward Model Operation

## Classification of Completions
- The reward model acts as a binary classifier, distinguishing between positive and negative classes based on human preferences.
- **Example:** For toxic content, the two classes could be "non-toxic" (positive) and "toxic" (negative).

## Logits and Reward Calculation
- The reward model provides raw model outputs, called logits, which are unnormalized values before applying any activation function (e.g., Softmax).
- The highest value for the positive class (e.g., "non-toxic") is used as the reward in RLHF.
![image](https://github.com/user-attachments/assets/b6da78c9-a65c-4504-af53-556d7fd9e9e3)

# Example: Detoxifying the LLM

## Toxicity Example
- If the model needs to identify hate speech, the positive class would be non-toxic content, and the negative class would be toxic content.
- A good reward would be given for non-toxic content, and a poor reward for toxic content.

## Softmax Application
- Applying the Softmax function to the logits converts them into probabilities for each class.
