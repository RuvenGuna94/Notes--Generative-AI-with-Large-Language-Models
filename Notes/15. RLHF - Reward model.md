# Training the Reward Model

## Purpose
- The reward model replaces human labelers once it has been trained.
- It automatically selects the preferred completion during the RLHF process.

## Reward Model as a Language Model
- The reward model is typically a language model, trained using supervised learning on the pairwise comparison data from human labelers (e.g., BERT).
- The model learns to favor the human-preferred completion and minimizes the difference between the reward values for different completions.

---

# Reward Model Operation

## Classification of Completions
- The reward model acts as a binary classifier, distinguishing between positive and negative classes based on human preferences.
- **Example:** For toxic content, the two classes could be "non-toxic" (positive) and "toxic" (negative).

## Logits and Reward Calculation
- The reward model provides raw model outputs, called logits, which are unnormalized values before applying any activation function (e.g., Softmax).
- The highest value for the positive class (e.g., "non-toxic") is used as the reward in RLHF.


