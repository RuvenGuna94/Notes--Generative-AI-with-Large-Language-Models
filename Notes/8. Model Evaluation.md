# Introduction
- **Challenge**: 
  - Evaluating LLM performance is complex due to the non-deterministic nature of language and the subjective quality of outputs.
- **Need for Metrics**: 
  - Quantify model performance and compare different models.

# Traditional Machine Learning vs. LLMs
- **Traditional ML**: 
  - Clear ground truth, deterministic outputs.
  - Simple metrics like accuracy are sufficient.
- **LLMs**: 
  - Non-deterministic outputs, subjective quality.
  - Requires more sophisticated evaluation methods. 

---

# Evaluation Metrics

## ROUGE (Recall Oriented Under Study for Gisting Evaluation)
- **Purpose**: Evaluate the quality of generated summaries by comparing them with human-generated summaries.
- **Explanation**:
  - The **recall** metric measures the number of words or unigrams that are matched between the reference and the generated output, divided by the number of words or unigrams in the reference.
  - **Precision** measures the unigram matches divided by the output size.
  - The **F1 score** is the harmonic mean of both these values.
- **Metrics**:
  - The number in the ROUGE metric corresponds to the number of n-grams or words being evaluated.
  - By utilizing 2 words for the evaluation, the order of the words is recognized; however, longer sentence generations will get penalized.
  - **ROUGE-1**: Based on unigram (single word) matches.
  - **ROUGE-2**: Based on bigram (two-word) matches.
  - **ROUGE-L**: Based on the longest common subsequence between the reference and the generated output.
- **Limitations**:
  - Sensitive to word order.
  - Can be misled by repetitions or irrelevant matches.
  - Requires careful interpretation and comparison within the same task.
- While different ROUGE scores can be used for experimentation, the most useful score depends on the sentence, sentence size, and use case.
![image](https://github.com/user-attachments/assets/c45d2b23-17ed-4d7b-a315-99e15ce17f18)
![image](https://github.com/user-attachments/assets/2fa9105e-e4b2-43c1-9e84-4a7b6ce06c5f)
![image](https://github.com/user-attachments/assets/7020a4bc-3cb1-4d20-b3e6-fb6173174290)

## BLEU (Bilingual Evaluation Understudy)
- **Purpose**: Evaluate the quality of machine translation (text translation).
- **Calculation**: Average precision over multiple n-gram sizes.
- **Limitations**: 
  - Does not directly measure fluency or grammatical correctness.

## Limitations of Both Metrics
- ROUGE and BLEU are relatively basic and low-cost metrics.
- They should not be used as the sole indicators of model quality.
