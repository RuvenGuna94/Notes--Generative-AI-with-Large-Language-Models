# Considerations for Deploying LLMs

### Functionality in Deployment
- How fast do you need your model to generate completions?
- What compute budget is available?
- Are you willing to trade performance for inference speed or lower storage?

### External Resources
- Will the model interact with external data or applications?
- How will connections to these resources be managed?

### Consumption of the Model
- What will the application or API interface for the model look like?

# Challenges of LLM Inference

### Key Issues
- High computing and storage requirements.
- Ensuring low latency for consuming applications.
- Deployment challenges across on-premise, cloud, and edge devices.

### Primary Goal
- Reduce model size while maintaining performance.

# Model Optimization Techniques

## Overview
- Techniques to improve inference performance and reduce resource usage.
- Focus: Distillation, Quantization, and Pruning.

## 1. Model Distillation

### Process
- Use a large teacher model to train a smaller student model.
- The student model statistically mimics the teacherâ€™s behavior.

### Steps
- Fine-tune the LLM (teacher model).
- Create a smaller student model.
- Freeze teacher weights and generate completions for training data.
- Use a loss function (distillation loss) to minimize differences between teacher and student outputs.
- The teacher model is already fine-tuned on the training data. So the probability distribution likely closely matches the ground truth data and won't have much variation in tokens.
- That's why Distillation adds a temperature parameter to the softmax function. With a temperature parameter greater than one, the probability distribution becomes broader and less strongly peaked.
- In parallel, you train the student model to generate the correct predictions based on your ground truth training data. Here, you don't vary the temperature setting and instead use the standard softmax function.
- The loss between these two is the student loss.
- The combined distillation and student losses are used to update the weights of the student model via backpropagation.

### Key Components
- **Soft Labels**: Teacher model outputs with higher temperature, providing broader distributions.
- **Hard Labels**: Ground truth data for student model training.
- **Soft Predictions**: Student model outputs mimicking soft labels.
- **Hard Predictions**: Student model outputs based on ground truth.

### Benefits
- Smaller student model is used for inference, reducing compute and storage costs.
- More effective for encoder-only models (e.g., BERT).
- Less effective for generative decoder models.

## 2. Quantization

### Post-Training Quantization (PTQ)
- Reduces model weights to lower precision (e.g., 16-bit floating point or 8-bit integer).
- Reduce model size, memory, and compute requirements for inference.
- Can be applied to model weights and/or activation.
- Quantization approaches that include the activations can have a higher impact on model performance.

### Steps
- Transform model weights and/or activation layers.
- Perform a calibration step to capture dynamic parameter ranges.

### Benefits
- Reduces memory and compute resources needed for serving.
- Can result in minor evaluation metric reductions, often worth cost savings.

### Tradeoffs
- Higher impact when both weights and activations are quantized.

## 3. Model Pruning

### Objective
- Eliminate model weights that contribute little to overall performance.

### Methods
- **Post-Training Pruning**: Remove near-zero weights after training.
- **Parameter-Efficient Fine-Tuning (e.g., LoRA)**: Adjust small subsets of parameters.

### Challenges
- Full retraining may be required.
- Pruning a small percentage of weights may have minimal impact.

# Summary of Techniques

### All three methods aim to:
- Reduce model size.
- Improve inference performance.
- Minimize impact on accuracy.

### Optimization Benefits
- Ensures efficient application performance.
- Enhances user experience.
