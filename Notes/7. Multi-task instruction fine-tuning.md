# Multitask Fine-Tuning

- **Concept**: Trains the model on a dataset containing examples for multiple tasks (e.g., summarization, translation, entity recognition).
- **Benefits**:
  - Improves performance across multiple tasks simultaneously.
  - Helps avoid catastrophic forgetting (degradation of performance on other tasks during single-task fine-tuning).
- **Drawbacks**:
  - Requires a large amount of data (50-100,000 examples).

---

# FLAN Models

- **Overview**:
  - Fine Tuned Language Net (FLAN)
  - Family of models fine-tuned using multitask instruction-based methods.
  - **Examples**:
    - **FLAN-T5**: The FLAN instruct version of the T5 foundation model.
      - Trained on a diverse set of 473 datasets across 146 tasks.
      - A strong general-purpose instruct model.
      - One example of a prompt dataset used for summarization tasks in FLAN-T5 is **SAMSum**:
        - Part of the muffin collection of tasks and datasets used to train language models to summarize dialogue.
        - SAMSum is a dataset with 16,000 messenger-like conversations with summaries.
    - **FLAN-PALM**: The FLAN instruct version of the PALM foundation model.
![image](https://github.com/user-attachments/assets/c3ce73c2-6927-4595-a5dc-146ced85fefd)
