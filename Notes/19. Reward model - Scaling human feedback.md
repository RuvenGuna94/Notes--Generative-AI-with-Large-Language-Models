# Challenges of Human Feedback in RLHF

## Resource Intensive
- Training a reward model requires significant human effort.
- Large labeled datasets require thousands of labelers evaluating numerous prompts.
- Time and resource demands are a limiting factor as models and use cases grow.

## Research Focus
- Scaling human feedback is an active area of research.
- One promising approach: Constitutional AI for self-supervised model training.

# Introduction to Constitutional AI

## Overview
- Proposed in 2022 by Anthropic researchers.
- Uses a set of rules and principles (constitution) to govern model behavior.
- Models self-critique and revise responses to comply with principles.

## Advantages
- Scales feedback by reducing reliance on human evaluations.
- Addresses unintended RLHF consequences, such as harmful responses due to over-prioritizing helpfulness.

# Example of RLHF Limitation

## Scenario: Prompt asks how to hack a neighbor’s WiFi.
- An aligned model, prioritizing helpfulness, might provide illegal information.
- Harmful behavior arises from misaligned priorities (e.g., helpfulness vs. harmlessness).

## Solution via Constitutional AI
- Rules help balance competing interests (e.g., helpfulness vs. ethics).
- **Example rule**: Prioritize harmlessness, avoiding responses that encourage illegal or unethical activity.

# Constitutional AI Implementation

## Phase 1: Supervised Learning

### Red Teaming
- Prompt the model to generate harmful responses deliberately.
- **Examples**: Hacking instructions or unethical suggestions.

### Self-Critique and Revision
- Model critiques harmful responses using constitutional principles.
- Generates revised, rule-compliant responses.

### Training Data Creation
- Red team prompts paired with constitutional responses form the training dataset.
- Fine-tune the model using these pairs.

### Example
- **Original Prompt**: How do I hack my neighbor’s WiFi?
- **Model’s Harmful Response**: Use App X to bypass security.
- **Critique**: Acknowledges the illegality of hacking.
- **Revised Response**: Declines to provide instructions, citing ethical concerns.

## Phase 2: Reinforcement Learning from AI Feedback (RLAIF)

### Process
- Fine-tuned model generates multiple responses to prompts.
- Model evaluates and selects preferred responses based on constitutional principles.

### Reward Model Training
- Use the preference dataset to train a reward model.
- Fine-tune the model further using PPO or similar RL algorithms.
![Uploading image.png…]()
