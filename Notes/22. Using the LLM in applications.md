# Key Issues with LLMs

1. **Knowledge Cutoff**
   - LLMs lack knowledge of events after their training period.
   - **Example**: A model trained in early 2022 might incorrectly state that Boris Johnson is the British Prime Minister.

2. **Math Struggles**
   - LLMs don't perform mathematical operations but predict the next best token.
   - **Example**: They may provide approximate but incorrect answers for division problems.

3. **Hallucination**
   - LLMs may generate information even when itâ€™s incorrect or fabricated.
   - **Example**: Inventing a nonexistent plant, like the "Martian Dunetree."

---

## Solution: Connecting LLMs to External Data Sources

- External data can improve accuracy and relevance during inference.
- **Example Tools**: LangChain for orchestration and connection to APIs.

---

# Retrieval Augmented Generation (RAG) Framework

### 1. Purpose
- Overcomes knowledge cutoff and hallucination issues.
- Provides access to external data sources at inference time.
- A framework (not specific technology) enabling LLMs to access unseen data during training.

### 2. Benefits
- Avoids expensive retraining.
- Enables the model to access up-to-date, proprietary, or context-specific data.


### 3. How It Works
- **Retriever** connects user input to external data sources:
  - **Query Encoder**: Encodes the user prompt into a query format.
  - **External Data Source**: Stores information (e.g., vector stores, databases, or documents).
- Relevant data is retrieved, combined with the user prompt, and passed to the LLM.

### 4. Example Use Case: Legal Discovery
- A lawyer queries court filings to find information about a plaintiff in a specific case.
- The retriever identifies relevant documents and expands the prompt with new information.
- The LLM generates an accurate, context-aware response.

---

# Advantages of RAG

1. **Access to Local Documents**
   - Private wikis, expert systems, or proprietary knowledge bases.

2. **Internet Access**
   - Enables real-time data retrieval, e.g., Wikipedia.

3. **Database Interaction**
   - Encodes the user prompt as a SQL query, allowing interaction with databases.

4. **Vector Stores**
   - Contains vector representations of text.
   - Enables fast and efficient search based on similarity (efficient semantic search using embedding vectors).

---

# Key Considerations

### 1. Context Window Limitations
- Most data sources exceed the token limit (general limit of 1000+ tokens).
- Data is divided into smaller chunks that fit within the context window.

### 2. Data Format
- Data must be structured for efficient retrieval.
- Embedding vectors allow semantically related text to be identified via similarity measures like cosine similarity.
- RAG methods process external data through LLMs to create embedding vectors, stored in **vector stores** for fast searching and efficient identification of related text.
