# Challenges of Full Fine-Tuning

- **High Memory Requirements:**  
  - Model weights (100+ GB for large models).  
  - Optimizer states, gradients, and forward activations consume significant memory (12-20x the size of model weights).  

---

# PEFT (Parameter-Efficient Fine-Tuning)

- Some techniques freeze most of the model weights and focus on fine-tuning a subset of existing model parameters (e.g., specific layers or components).  
- Other techniques avoid altering the original model weights and instead add a small number of new parameters or layers, fine-tuning only these new components.  
- **Advantages:**  
  - Reduces memory requirements by freezing most or all of the original LLM parameters.  
  - The number of trained parameters is much smaller (in some cases, just 15-20% of the original LLM weights).  
  - Enables training on limited hardware (e.g., single GPU).  
  - Less prone to catastrophic forgetting since most model weights are preserved.  
  - Reduces storage costs by storing only the small set of trained parameters.  

---

# PEFT Methods

### Selective Methods
- Fine-tune specific subsets of parameters (e.g., certain layers or parameter types).  
- Mixed performance with significant trade-offs.  

### Reparameterization Methods
- Reduce the number of trainable parameters by creating low-rank transformations of original weights.  
- Example:
  - LoRA (Low-Rank Adaptation of Large Language Models).  

### Additive Methods
- **Adapter Methods:**  
  - Add new trainable layers (adapters) to the model architecture.  
- **Soft Prompt Methods:**  
  - Modify the input to the model (e.g., by training prompt embeddings) without altering the model itself.  
  - **Prompt Tuning:** Modifying model weights to get better performance on specific prompts.  
