# Challenges of Full Fine-Tuning

- **High Memory Requirements:**  
  - Model weights (100+ GB for large models).  
  - Optimizer states, gradients, and forward activations consume significant memory (12-20x the size of model weights).  

---

# PEFT (Parameter-Efficient Fine-Tuning)

- Some techniques freeze most of the model weights and focus on fine-tuning a subset of existing model parameters (e.g., specific layers or components).  
- Other techniques avoid altering the original model weights and instead add a small number of new parameters or layers, fine-tuning only these new components.  
- **Advantages:**  
  - Reduces memory requirements by freezing most or all of the original LLM parameters.  
  - The number of trained parameters is much smaller (in some cases, just 15-20% of the original LLM weights).  
  - Enables training on limited hardware (e.g., single GPU).  
  - Less prone to catastrophic forgetting since most model weights are preserved.  
  - Reduces storage costs by storing only the small set of trained parameters.  


