# Reinforcement Learning from Human Feedback (RLHF)

## Overview
- Combines reinforcement learning (RL) with human feedback data.
- Aligns outputs with human preferences, ensuring relevance and usefulness.
- Helps minimize harmful outputs by training the model to acknowledge limitations and avoid toxic content.

## Applications
- Personalization of large language models (LLMs):
  - Models learn individual user preferences through continuous feedback.
  - Potential for personalized AI assistants and individualized learning plans.

# Reinforcement Learning Basics

## Core Concepts
- The agent takes actions, observes changes, and receives rewards/penalties.
- Through iterative trial and error, the agent refines its strategy to maximize cumulative rewards.
- The goal of RL is for the agent (or model) to learn the optimal policy for a given environment that maximizes rewards.
- Initial action is random. From this state, the agent proceeds to explore subsequent states through further actions. The series of actions and corresponding states form a playout, often called a rollout.
![image](https://github.com/user-attachments/assets/5a488646-5855-48c0-a50e-42172097d33b)

# RLHF in Fine-Tuning LLMs

## Components
- **Agent:** LLM guided by a policy to generate aligned text.
- **Environment:** Context window (input prompt space).
- **State:** Current context (input text in the window).
- **Action:** Generating text (word, sentence, or paragraph).
- **Action Space:** Token vocabulary used to construct outputs.

## Process
- Text generation depends on learned statistical representations (probability distribution) of language.
- Rewards are assigned based on alignment with human preferences (e.g., helpfulness, accuracy, non-toxicity).
![image](https://github.com/user-attachments/assets/43842f96-2d57-411d-b9a8-a56355338832)

# Determining Rewards

## Human Feedback
- Human evaluators score outputs based on alignment metrics (e.g., toxicity).
- Scalar values (e.g., 0 or 1) represent feedback.

## Reward Model
- A trained model replaces humans for scalable evaluations.
- Encodes preferences learned from human feedback.
- Training: Initial supervised learning on human-labeled examples.
- Role: Assesses LLM outputs and assigns reward values.
- Rewards are used to update the LLM weights iteratively.
