# Reinforcement Learning from Human Feedback (RLHF)

## Overview
- Combines reinforcement learning (RL) with human feedback data.
- Aligns outputs with human preferences, ensuring relevance and usefulness.
- Helps minimize harmful outputs by training the model to acknowledge limitations and avoid toxic content.

## Applications
- Personalization of large language models (LLMs):
  - Models learn individual user preferences through continuous feedback.
  - Potential for personalized AI assistants and individualized learning plans.

# Reinforcement Learning Basics

## Core Concepts
- The agent takes actions, observes changes, and receives rewards/penalties.
- Through iterative trial and error, the agent refines its strategy to maximize cumulative rewards.
- The goal of RL is for the agent (or model) to learn the optimal policy for a given environment that maximizes rewards.
- Initial action is random. From this state, the agent proceeds to explore subsequent states through further actions. The series of actions and corresponding states form a playout, often called a rollout.

# RLHF in Fine-Tuning LLMs

## Components
- **Agent:** LLM guided by a policy to generate aligned text.
- **Environment:** Context window (input prompt space).
- **State:** Current context (input text in the window).
- **Action:** Generating text (word, sentence, or paragraph).
- **Action Space:** Token vocabulary used to construct outputs.

## Process
- Text generation depends on learned statistical representations (probability distribution) of language.
- Rewards are assigned based on alignment with human preferences (e.g., helpfulness, accuracy, non-toxicity).


