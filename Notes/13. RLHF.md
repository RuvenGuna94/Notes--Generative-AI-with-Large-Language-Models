# Reinforcement Learning from Human Feedback (RLHF)

## Overview
- Combines reinforcement learning (RL) with human feedback data.
- Aligns outputs with human preferences, ensuring relevance and usefulness.
- Helps minimize harmful outputs by training the model to acknowledge limitations and avoid toxic content.

## Applications
- Personalization of large language models (LLMs):
  - Models learn individual user preferences through continuous feedback.
  - Potential for personalized AI assistants and individualized learning plans.

# Reinforcement Learning Basics

## Core Concepts
- The agent takes actions, observes changes, and receives rewards/penalties.
- Through iterative trial and error, the agent refines its strategy to maximize cumulative rewards.
- The goal of RL is for the agent (or model) to learn the optimal policy for a given environment that maximizes rewards.
- Initial action is random. From this state, the agent proceeds to explore subsequent states through further actions. The series of actions and corresponding states form a playout, often called a rollout.