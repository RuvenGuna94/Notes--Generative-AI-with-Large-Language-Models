# Scaling Laws

- **Increasing dataset size**: More data can improve model performance.
- **Increasing model parameters**: Larger models can learn more complex patterns.
- **Compute budget limitations**: Consider available GPUs and training time.

![image](https://github.com/user-attachments/assets/6c03e24e-345c-44ee-868a-1aecb1cf5713)

---

## Unit of Compute
- **PetaFLOP per second day**:
  - One quadrillion floating-point operations per second for 24 hours.
  - Approximately equivalent to 8 NVIDIA V100 GPUs running at full capacity for one day.
  - More powerful processors can reduce the number of chips required.
  - For example, two NVIDIA A100 GPUs provide equivalent compute to the eight V100 chips.

## Scaling Laws and Compute Budget
- **Relationship between compute budget, model size, and dataset size**:
  - Larger models and datasets generally require more compute.
  - Power-law relationship between compute budget and model performance.
  - Compute budget is often a hard constraint.

---

## Model Scaling and Performance
- **Trade-offs between model size and dataset size**:
  - Increasing either can improve performance within a fixed compute budget.
  - Optimal balance depends on specific use cases and available resources.

---

## Chinchilla and Compute-Optimal Models
- **Chinchilla paper**:
  - Investigated the relationship between model size, dataset size, and compute budget.
  - Found that many large language models are overparameterized and undertrained.
  - Proposed that smaller models can achieve similar performance with larger datasets.
  - Optimal dataset size is approximately 20 times the number of model parameters.

---

## Future Trends
- **Shift towards compute-optimal models**:
  - Smaller, more efficient models can achieve comparable performance.
  - Pre-training from scratch may be necessary in certain cases.