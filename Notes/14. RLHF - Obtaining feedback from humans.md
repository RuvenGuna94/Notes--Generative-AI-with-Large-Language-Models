# Fine-Tuning LLMs with RLHF

## Step 1: Select a Model
- Choose an LLM that can handle the desired task (e.g., text summarization, question answering).
- Consider starting with a general model such as an instruct model, which is pre-fine-tuned on various tasks.

## Step 2: Prepare a Prompt Dataset
- Use the selected model to generate multiple responses for each prompt.
- The prompt dataset consists of prompts that the LLM processes to generate a set of completions.

## Step 3: Collect Human Feedback
- Human labelers assess the completions generated by the model based on specific criteria (e.g., helpfulness, toxicity).
- Example Prompt: "My house is too hot"
  - The LLM generates three completions, and labelers rank them based on helpfulness.
  - Labelers rank the most helpful completion first, and the least helpful second or third.

### Labeling Criteria
- Decide what the labelers should assess (e.g., helpfulness, accuracy).
- Labelers should be consistent in their rankings, establishing consensus across multiple labelers.

### Instructions for Labelers
- Provide clear, detailed instructions to ensure high-quality feedback and consistency.
- Instruct labelers on how to assess responses (e.g., based on correctness and informativeness) and how to handle ties or irrelevant answers.

### Example Instructions
- Labelers can use the Internet to fact-check.
- In case of tie rankings, labelers should rank them the same but do so sparingly.
- If a response is nonsensical, labelers should select "F" (for fail) rather than rank, to flag poor-quality answers.

