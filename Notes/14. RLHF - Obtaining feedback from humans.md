# Fine-Tuning LLMs with RLHF

## Step 1: Select a Model
- Choose an LLM that can handle the desired task (e.g., text summarization, question answering).
- Consider starting with a general model such as an instruct model, which is pre-fine-tuned on various tasks.

## Step 2: Prepare a Prompt Dataset
- Use the selected model to generate multiple responses for each prompt.
- The prompt dataset consists of prompts that the LLM processes to generate a set of completions.

## Step 3: Collect Human Feedback
- Human labelers assess the completions generated by the model based on specific criteria (e.g., helpfulness, toxicity).
- Example Prompt: "My house is too hot"
  - The LLM generates three completions, and labelers rank them based on helpfulness.
  - Labelers rank the most helpful completion first, and the least helpful second or third.


