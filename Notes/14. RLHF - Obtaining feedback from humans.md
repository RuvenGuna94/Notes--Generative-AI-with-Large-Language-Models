# Fine-Tuning LLMs with RLHF

## Step 1: Select a Model
- Choose an LLM that can handle the desired task (e.g., text summarization, question answering).
- Consider starting with a general model such as an instruct model, which is pre-fine-tuned on various tasks.

## Step 2: Prepare a Prompt Dataset
- Use the selected model to generate multiple responses for each prompt.
- The prompt dataset consists of prompts that the LLM processes to generate a set of completions.

## Step 3: Collect Human Feedback
- Human labelers assess the completions generated by the model based on specific criteria (e.g., helpfulness, toxicity).
- Example Prompt: "My house is too hot"
  - The LLM generates three completions, and labelers rank them based on helpfulness.
  - Labelers rank the most helpful completion first, and the least helpful second or third.

### Labeling Criteria
- Decide what the labelers should assess (e.g., helpfulness, accuracy).
- Labelers should be consistent in their rankings, establishing consensus across multiple labelers.

### Instructions for Labelers
- Provide clear, detailed instructions to ensure high-quality feedback and consistency.
- Instruct labelers on how to assess responses (e.g., based on correctness and informativeness) and how to handle ties or irrelevant answers.

### Example Instructions
- Labelers can use the Internet to fact-check.
- In case of tie rankings, labelers should rank them the same but do so sparingly.
- If a response is nonsensical, labelers should select "F" (for fail) rather than rank, to flag poor-quality answers.
![image](https://github.com/user-attachments/assets/ae5b62ed-ef9e-4b7c-abeb-b0e9e9f3a1b6)
![image](https://github.com/user-attachments/assets/cc7f06c8-bd6d-41d5-a421-df10326f3c2c)

---

# Training the Reward Model

## Step 4: Convert Rankings to Pairwise Comparisons
- For each prompt, convert rankings into pairwise comparisons of completions.
- Each pair of responses gets assigned a reward:
  - `1` for the preferred completion.
  - `0` for the less preferred completion.
![image](https://github.com/user-attachments/assets/aef9b76e-5d3d-4946-84ce-cbebfc66f54c)

## Step 5: Format Data for Reward Model
- Reorder responses so the preferred option comes first.
- Use this data to train the reward model that will classify completions during reinforcement learning fine-tuning.

---

# Ranked vs. Thumbs-Up Feedback

## Ranking Feedback
- Provides more data for training the reward model compared to simpler thumbs-up/down feedback.
- With N completions per prompt, you get N choose 2 combinations, increasing the dataset for training.
