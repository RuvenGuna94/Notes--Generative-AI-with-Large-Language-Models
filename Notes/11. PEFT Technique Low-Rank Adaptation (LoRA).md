# Introduction

## Parameter-Efficient Fine-Tuning Technique
- Falls under the category of **reparameterization methods**.  
- Reduces the number of trainable parameters during fine-tuning.  

## Background
- After the prompt is converted into tokens, it is passed through an embedding layer, then to the encoder and decoder.  
- Both encoder and decoder have neural networks called the **self-attention** and **feed-forward network**, where weights are learned during pre-training.  
- **LoRA** reduces the number of trainable parameters in the self-attention layer.  
  - While LoRA can also be applied to other components like feed-forward layers, the majority of parameters in LLMs reside in the attention layers, making them the primary target for parameter savings.  

## Key Concept
- **Rank** relates to the amount of trainable parameters LoRA will use during training.  
- Injects a pair of **low-rank matrices** into the original model's weight matrices.  
- Trains only these smaller matrices while keeping the original weights frozen.  

### Rank Decomposition
- Introduces two smaller matrices (**A** and **B**) whose product matches the dimensions of the original weight matrix.  
- Trainable parameters are significantly reduced.  

### Inference
- Multiply **A** and **B** to obtain a matrix.  
- Add this matrix to the original frozen weights.  
- Use the updated weights for inference.  

- **Final Model:**  
  - Retains the same number of parameters as the original model.  
  - Little to no impact on inference latency.  

---

# Benefits

## Reduced Memory Requirements
- Trains a much smaller number of parameters.  
- Enables training on limited hardware (e.g., single GPU).  

## Improved Efficiency
- Faster training process.  

## Minimal Inference Latency
- Minimal impact on inference speed as the total number of parameters remains nearly unchanged.  

## Reduced Storage Costs
- Store only the small LoRA matrices for each task.  


