# Introduction

## Parameter-Efficient Fine-Tuning Technique
- Falls under the category of **reparameterization methods**.  
- Reduces the number of trainable parameters during fine-tuning.  

## Background
- After the prompt is converted into tokens, it is passed through an embedding layer, then to the encoder and decoder.  
- Both encoder and decoder have neural networks called the **self-attention** and **feed-forward network**, where weights are learned during pre-training.  
- **LoRA** reduces the number of trainable parameters in the self-attention layer.  
  - While LoRA can also be applied to other components like feed-forward layers, the majority of parameters in LLMs reside in the attention layers, making them the primary target for parameter savings.  


