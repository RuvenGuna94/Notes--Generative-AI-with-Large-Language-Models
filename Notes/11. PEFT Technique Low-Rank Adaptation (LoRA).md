# Introduction

## Parameter-Efficient Fine-Tuning Technique
- Falls under the category of **reparameterization methods**.  
- Reduces the number of trainable parameters during fine-tuning.  

## Background
- After the prompt is converted into tokens, it is passed through an embedding layer, then to the encoder and decoder.  
- Both encoder and decoder have neural networks called the **self-attention** and **feed-forward network**, where weights are learned during pre-training.  
- **LoRA** reduces the number of trainable parameters in the self-attention layer.  
  - While LoRA can also be applied to other components like feed-forward layers, the majority of parameters in LLMs reside in the attention layers, making them the primary target for parameter savings.  

## Key Concept
- **Rank** relates to the amount of trainable parameters LoRA will use during training.  
- Injects a pair of **low-rank matrices** into the original model's weight matrices.  
- Trains only these smaller matrices while keeping the original weights frozen.  

### Rank Decomposition
- Introduces two smaller matrices (**A** and **B**) whose product matches the dimensions of the original weight matrix.  
- Trainable parameters are significantly reduced.  

### Inference
- Multiply **A** and **B** to obtain a matrix.  
- Add this matrix to the original frozen weights.  
- Use the updated weights for inference.  

- **Final Model:**  
  - Retains the same number of parameters as the original model.  
  - Little to no impact on inference latency.  

---

# Benefits

## Reduced Memory Requirements
- Trains a much smaller number of parameters.  
- Enables training on limited hardware (e.g., single GPU).  

## Improved Efficiency
- Faster training process.  

## Minimal Inference Latency
- Minimal impact on inference speed as the total number of parameters remains nearly unchanged.  

## Reduced Storage Costs
- Store only the small LoRA matrices for each task.  

---

# Application

## Typically Applied to Self-Attention Layers
- Provides the most significant reduction in trainable parameters.  

## Example
- For a **512x64 weight matrix** (transformer architecture described in *Attention is All You Need*):  
  - **LoRA with rank 8:**  
    - Reduces trainable parameters from **32,768** to **4,608** (86% reduction compared to full fine-tuning).  
      - Matrix **A**: 8x64 = 512 parameters.  
      - Matrix **B**: 512x8 = 4,096 parameters.  

- Multiple LoRA matrices can be utilized for different tasks on the same model.  
  - Memory required to store these matrices is very small, allowing for training across many tasks without storing multiple full-sized versions of the base LLM.  

---


